# ç¬¬2ç«  ONNX Runtimeã¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™º

> æœ¬ç« ã®æ¦‚è¦: ONNX Runtimeã®å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€ONNXã®åŸºæœ¬åŸç†ã€Pythoné€£æºã€æ¼”ç®—å­å±æ€§ãªã©ã€é–‹ç™ºã«å¿…è¦ãªè¦ç‚¹ã‚’å®Ÿè·µã‚³ãƒ¼ãƒ‰ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚

å‰ç« ã§ONNX Runtimeã®åŸºæœ¬ã‚’å­¦ã‚“ã ã®ã§ã€ã“ã“ã§ã¯å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å®Ÿè·µçš„ãªé–‹ç™ºæ‰‹æ³•ã‚’æ˜ã‚Šä¸‹ã’ã¾ã™ã€‚

## 2.1 å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆExecution Providersï¼‰

ONNX Runtimeã®ç‰¹å¾´ã®ã²ã¨ã¤ãŒ**å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**ã§ã™ã€‚åŒä¸€ã®ONNXãƒ¢ãƒ‡ãƒ«ã‚’ã€ã•ã¾ã–ã¾ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä¸Šã§æœ€é©åŒ–ã—ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³æ©Ÿæ§‹ã§ã™ã€‚

### 2.1.1 å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æ¦‚è¦

å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã€ONNX Runtimeã®ã‚³ã‚¢ã‚’æ§‹æˆã™ã‚‹é‡è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ç‰¹å®šã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢/ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚

**å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ä¸»ãªå½¹å‰²:**
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å›ºæœ‰ã®æœ€é©åŒ–ã®é©ç”¨
- ãƒ¡ãƒ¢ãƒªç®¡ç†ã®åŠ¹ç‡åŒ–
- ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–
- æ¼”ç®—å­ã®é«˜é€Ÿå®Ÿè£…ã®æä¾›

**ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã®åŸç†:**
ONNX Runtimeã¯ã€æŒ‡å®šé †ã«å„æ¼”ç®—å­ã¸æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚æœªå¯¾å¿œã®æ¼”ç®—å­ã¯æ¬¡ç‚¹ã¸è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚

#### ä¸»è¦ãªå®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

ã‚·ã‚¹ãƒ†ãƒ ã«åˆ©ç”¨å¯èƒ½ãªå®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèªã—ã€é©åˆ‡ã«è¨­å®šã™ã‚‹ã“ã¨ãŒé«˜æ€§èƒ½ãªæ¨è«–ã®ç¬¬ä¸€æ­©ã§ã™ã€‚

```python
import onnxruntime as ort
import sys
import platform

def analyze_available_providers():
    """ã‚·ã‚¹ãƒ†ãƒ ã§åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©³ç´°ã«åˆ†æ"""
    
    print("=== ONNX Runtime ç’°å¢ƒåˆ†æ ===")
    print(f"ONNX Runtime ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {ort.__version__}")
    print(f"Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.platform()}")
    
    providers = ort.get_available_providers()
    print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ•°: {len(providers)}")
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è©³ç´°åˆ†æ
    for i, provider in enumerate(providers, 1):
        print(f"\n{i}. {provider}")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç‰¹æ€§ã‚’è§£èª¬
        if provider == "CPUExecutionProvider":
            print("   ğŸ–¥ï¸  CPUä¸Šã§ã®æ±ç”¨å®Ÿè¡Œï¼ˆå¸¸ã«åˆ©ç”¨å¯èƒ½ï¼‰")
            print("   ğŸ’¡ ç‰¹å¾´: ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€å®‰å®šæ€§é‡è¦–")
        elif provider == "CUDAExecutionProvider":
            print("   ğŸš€ NVIDIA GPUä¸Šã§ã®é«˜é€Ÿå®Ÿè¡Œ")
            print("   ğŸ’¡ ç‰¹å¾´: æ·±å±¤å­¦ç¿’ã«æœ€é©ã€é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ")
        elif provider == "DmlExecutionProvider":
            print("   ğŸ® DirectMLï¼ˆWindows GPUï¼‰ã§ã®å®Ÿè¡Œ")
            print("   ğŸ’¡ ç‰¹å¾´: Windowsã®GPUæŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼")
        elif provider == "TensorrtExecutionProvider":
            print("   âš¡ NVIDIA TensorRTã§ã®æœ€é©åŒ–å®Ÿè¡Œ")
            print("   ğŸ’¡ ç‰¹å¾´: æœ¬ç•ªç’°å¢ƒã§ã®æœ€é«˜æ€§èƒ½")
        elif provider == "OpenVINOExecutionProvider":
            print("   ğŸ”§ Intel OpenVINOã§ã®æœ€é©åŒ–å®Ÿè¡Œ")
            print("   ğŸ’¡ ç‰¹å¾´: Intel CPU/GPU/VPUã§ã®é«˜é€ŸåŒ–")

# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆé–¢æ•°ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰
def create_session_with_provider(model_path, provider_name, provider_options=None):
    """æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    try:
        if provider_options:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            providers = [(provider_name, provider_options)]
        else:
            providers = [provider_name]
            
        session = ort.InferenceSession(model_path, providers=providers)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸæ™‚ã®è©³ç´°æƒ…å ±
        print(f"âœ… {provider_name}ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
        print(f"   å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {session.get_providers()}")
        
        return session
        
    except Exception as e:
        print(f"âŒ {provider_name}ã§ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
        print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}: {e}")
        return None

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def test_providers(model_path="model.onnx"):
    """å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("\n=== ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ ===")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆ†æ
    analyze_available_providers()
    
    available_providers = ort.get_available_providers()
    sessions = {}
    
    # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚’è©¦è¡Œ
    for provider in available_providers:
        print(f"\nğŸ”¬ {provider} ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
        session = create_session_with_provider(model_path, provider)
        if session:
            sessions[provider] = session
    
    return sessions

# å®Ÿè¡Œ
if __name__ == "__main__":
    # æ³¨æ„: å®Ÿéš›ã®ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™
    # test_providers("your_model.onnx")
    analyze_available_providers()
```

**ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã®å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:**

1. **é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆæ®µéš**: 
   - `CPUExecutionProvider`ã‚’ä½¿ç”¨ï¼ˆç¢ºå®Ÿã«å‹•ä½œï¼‰
   
2. **é«˜æ€§èƒ½ãŒå¿…è¦ãªå ´åˆ**: 
   - GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰`CUDAExecutionProvider`ã‚’æœ€å„ªå…ˆ
   - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«`CPUExecutionProvider`ã‚’ä½µè¨˜

3. **æœ¬ç•ªç’°å¢ƒã§ã®æœ€é©åŒ–**: 
   - `TensorrtExecutionProvider`ï¼ˆNVIDIA GPUï¼‰
   - `OpenVINOExecutionProvider`ï¼ˆIntelç’°å¢ƒï¼‰

### 2.1.2 ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è©³ç´°

å„å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã¯ã€ãã‚Œãã‚Œå›ºæœ‰ã®ç‰¹æ€§ã¨æœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚é©åˆ‡ãªè¨­å®šã«ã‚ˆã‚Šã€æ¨è«–æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

#### 1. CPUExecutionProvider

CPUExecutionProviderã¯ã€æœ€ã‚‚æ±ç”¨æ€§ãŒé«˜ãã€ã™ã¹ã¦ã®ç’°å¢ƒã§åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã™ã€‚ãƒãƒ«ãƒã‚³ã‚¢CPUã®æ€§èƒ½ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹ãŸã‚ã®ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚’æä¾›ã—ã¾ã™ã€‚

**ä¸»ãªæ©Ÿèƒ½:**
- **Intel MKL-DNNçµ±åˆ**: Intel CPUã§ã®é«˜é€ŸåŒ–
- **OpenMPä¸¦åˆ—å‡¦ç†**: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œã®æœ€é©åŒ–
- **SIMDå‘½ä»¤æ´»ç”¨**: ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã®é«˜é€ŸåŒ–
- **ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã®å‘ä¸Š

```python
# CPUå®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šä¾‹
cpu_options = {
    'intra_op_num_threads': 8,  # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼å†…ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
    'inter_op_num_threads': 4,  # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼é–“ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
}

session = ort.InferenceSession(
    "model.onnx",
    providers=[('CPUExecutionProvider', cpu_options)]
)
```

#### 2. CUDAExecutionProvider
NVIDIA GPU ã§ã®é«˜é€Ÿå‡¦ç†ï¼š

```python
# CUDAå®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šä¾‹
cuda_options = {
    'device_id': 0,                    # ä½¿ç”¨ã™ã‚‹GPU ID
    'arena_extend_strategy': 'kNextPowerOfTwo',
    'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # GPU ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆ2GBï¼‰
    'cudnn_conv_algo_search': 'EXHAUSTIVE',    # æœ€é©ãªconvç®—æ³•ã‚’æ¤œç´¢
}

session = ort.InferenceSession(
    "model.onnx",
    providers=[('CUDAExecutionProvider', cuda_options)]
)
```

#### 3. ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

```python
# DirectMLï¼ˆDirectX 12ï¼‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ - Windows
dml_options = {'device_id': 0}
dml_session = ort.InferenceSession(
    "model.onnx",
    providers=[('DmlExecutionProvider', dml_options)]
)

# TensorRT ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ - NVIDIA GPUæœ€é©åŒ–
trt_options = {
    'device_id': 0,
    'trt_max_workspace_size': 1073741824,  # 1GB
    'trt_fp16_enable': True,               # FP16ç²¾åº¦ã‚’æœ‰åŠ¹åŒ–
}
```

### 2.1.3 ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¿½åŠ 

#### ã‚«ã‚¹ã‚¿ãƒ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿

```python
import onnxruntime as ort

# ã‚«ã‚¹ã‚¿ãƒ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
session_options = ort.SessionOptions()
session_options.register_custom_ops_library("/path/to/custom_ops.so")

session = ort.InferenceSession("model_with_custom_ops.onnx", session_options)
```

#### ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å„ªå…ˆé †ä½ã®è¨­å®š

```python
# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å„ªå…ˆé †ä½ã‚’è¨­å®š
providers = [
    'TensorrtExecutionProvider',  # æœ€é«˜å„ªå…ˆåº¦
    'CUDAExecutionProvider',      # æ¬¡ã®å„ªå…ˆåº¦
    'CPUExecutionProvider'        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
]

session = ort.InferenceSession("model.onnx", providers=providers)
```

## 2.2ã€€ONNXåŸç†ã®ç´¹ä»‹

### 2.2.1ã€€ONNXåŸºæœ¬æ¦‚å¿µ

ONNXã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ¨™æº–åŒ–ã•ã‚ŒãŸè¡¨ç¾å½¢å¼ã§ã™ã€‚ä¸»è¦ãªæ¦‚å¿µã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚

#### ONNXãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ 

```python
import onnx

# ONNXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model = onnx.load("model.onnx")

# ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬æƒ…å ±
print(f"IR ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model.ir_version}")
print(f"ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼å: {model.producer_name}")
print(f"ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model.producer_version}")

# ã‚°ãƒ©ãƒ•ã®æƒ…å ±
graph = model.graph
print(f"ãƒãƒ¼ãƒ‰æ•°: {len(graph.node)}")
print(f"å…¥åŠ›æ•°: {len(graph.input)}")
print(f"å‡ºåŠ›æ•°: {len(graph.output)}")
print(f"åˆæœŸåŒ–å­æ•°: {len(graph.initializer)}")
```

#### ãƒ¢ãƒ‡ãƒ«ã®å¯è¦–åŒ–

```python
import onnx
from onnx import helper, numpy_helper

def print_model_structure(model_path):
    """ONNXãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’è¡¨ç¤º"""
    model = onnx.load(model_path)
    
    print("=== ONNX ãƒ¢ãƒ‡ãƒ«æ§‹é€  ===")
    
    # å…¥åŠ›æƒ…å ±
    print("\n--- å…¥åŠ› ---")
    for input_tensor in model.graph.input:
        print(f"åå‰: {input_tensor.name}")
        print(f"ãƒ‡ãƒ¼ã‚¿å‹: {input_tensor.type.tensor_type.elem_type}")
        shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]
        print(f"å½¢çŠ¶: {shape}")
    
    # å‡ºåŠ›æƒ…å ±
    print("\n--- å‡ºåŠ› ---")
    for output_tensor in model.graph.output:
        print(f"åå‰: {output_tensor.name}")
        print(f"ãƒ‡ãƒ¼ã‚¿å‹: {output_tensor.type.tensor_type.elem_type}")
        shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]
        print(f"å½¢çŠ¶: {shape}")
    
    # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼æƒ…å ±
    print("\n--- ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ ---")
    op_types = {}
    for node in model.graph.node:
        op_type = node.op_type
        op_types[op_type] = op_types.get(op_type, 0) + 1
    
    for op_type, count in sorted(op_types.items()):
        print(f"{op_type}: {count}å€‹")

# ä½¿ç”¨ä¾‹
print_model_structure("model.onnx")
```

### 2.2.2ã€€ONNXã®å…¥åŠ›ã€å‡ºåŠ›ã€ãƒãƒ¼ãƒ‰ã€åˆæœŸåŒ–å­ã€å±æ€§

#### ãƒãƒ¼ãƒ‰ï¼ˆNodeï¼‰ã®è©³ç´°åˆ†æ

```python
import onnx

def analyze_nodes(model_path):
    """ãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’åˆ†æ"""
    model = onnx.load(model_path)
    
    print("=== ãƒãƒ¼ãƒ‰åˆ†æ ===")
    for i, node in enumerate(model.graph.node):
        print(f"\nãƒãƒ¼ãƒ‰ {i+1}:")
        print(f"  åå‰: {node.name}")
        print(f"  ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—: {node.op_type}")
        print(f"  å…¥åŠ›: {list(node.input)}")
        print(f"  å‡ºåŠ›: {list(node.output)}")
        
        # å±æ€§ã®è¡¨ç¤º
        if node.attribute:
            print(f"  å±æ€§:")
            for attr in node.attribute:
                print(f"    {attr.name}: {helper.get_attribute_value(attr)}")

# åˆæœŸåŒ–å­ï¼ˆInitializerï¼‰ã®åˆ†æ
def analyze_initializers(model_path):
    """åˆæœŸåŒ–å­ã®è©³ç´°ã‚’åˆ†æ"""
    model = onnx.load(model_path)
    
    print("=== åˆæœŸåŒ–å­åˆ†æ ===")
    for initializer in model.graph.initializer:
        print(f"\nåˆæœŸåŒ–å­: {initializer.name}")
        print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {initializer.data_type}")
        print(f"  å½¢çŠ¶: {list(initializer.dims)}")
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå°ã•ãªãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆã®ã¿ï¼‰
        if numpy_helper.to_array(initializer).size <= 10:
            data = numpy_helper.to_array(initializer)
            print(f"  ãƒ‡ãƒ¼ã‚¿: {data.flatten()}")

# ä½¿ç”¨ä¾‹
analyze_nodes("model.onnx")
analyze_initializers("model.onnx")
```

### 2.2.3ã€€è¦ç´ ã‚¿ã‚¤ãƒ—

ONNXã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®è©³ç´°ï¼š

```python
from onnx import TensorProto

# ONNXãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ä¸€è¦§
data_types = {
    TensorProto.FLOAT: "float32",
    TensorProto.UINT8: "uint8",
    TensorProto.INT8: "int8",
    TensorProto.UINT16: "uint16",
    TensorProto.INT16: "int16",
    TensorProto.INT32: "int32",
    TensorProto.INT64: "int64",
    TensorProto.STRING: "string",
    TensorProto.BOOL: "bool",
    TensorProto.FLOAT16: "float16",
    TensorProto.DOUBLE: "float64",
    TensorProto.UINT32: "uint32",
    TensorProto.UINT64: "uint64",
}

def check_model_data_types(model_path):
    """ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª"""
    model = onnx.load(model_path)
    used_types = set()
    
    # å…¥åŠ›ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
    for input_tensor in model.graph.input:
        elem_type = input_tensor.type.tensor_type.elem_type
        used_types.add(elem_type)
    
    # åˆæœŸåŒ–å­ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
    for initializer in model.graph.initializer:
        used_types.add(initializer.data_type)
    
    print("ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—:")
    for type_id in used_types:
        type_name = data_types.get(type_id, f"Unknown({type_id})")
        print(f"  {type_name}")

check_model_data_types("model.onnx")
```

### 2.2.4ã€€opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã¯ï¼Ÿ

ONNXã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆopsetï¼‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼š

```python
import onnx

def check_opset_version(model_path):
    """ãƒ¢ãƒ‡ãƒ«ã®opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª"""
    model = onnx.load(model_path)
    
    print("=== Opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ± ===")
    for opset_import in model.opset_import:
        domain = opset_import.domain if opset_import.domain else "default"
        version = opset_import.version
        print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}, ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {version}")
    
    return model

# opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å¤‰æ›
def convert_opset_version(input_model_path, output_model_path, target_version):
    """opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¤‰æ›"""
    try:
        from onnx import version_converter
        
        original_model = onnx.load(input_model_path)
        converted_model = version_converter.convert_version(original_model, target_version)
        onnx.save(converted_model, output_model_path)
        
        print(f"Opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’{target_version}ã«å¤‰æ›ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")

# ä½¿ç”¨ä¾‹
check_opset_version("model.onnx")
convert_opset_version("model.onnx", "model_v11.onnx", 11)
```

### 2.2.5ã€€ã‚µãƒ–ã‚°ãƒ©ãƒ•ã€ãƒ†ã‚¹ãƒˆã€ãƒ«ãƒ¼ãƒ—

#### åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æ¼”ç®—å­ã®ç†è§£

```python
import onnx
from onnx import helper

def analyze_control_flow(model_path):
    """åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æ¼”ç®—å­ã‚’åˆ†æ"""
    model = onnx.load(model_path)
    
    control_flow_ops = ['If', 'Loop', 'Scan']
    
    print("=== åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼åˆ†æ ===")
    for node in model.graph.node:
        if node.op_type in control_flow_ops:
            print(f"\nåˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æ¼”ç®—å­ç™ºè¦‹: {node.op_type}")
            print(f"ãƒãƒ¼ãƒ‰å: {node.name}")
            
            # ã‚µãƒ–ã‚°ãƒ©ãƒ•ã®åˆ†æ
            for attr in node.attribute:
                if attr.type == onnx.AttributeProto.GRAPH:
                    print(f"ã‚µãƒ–ã‚°ãƒ©ãƒ•å±æ€§: {attr.name}")
                    subgraph = attr.g
                    print(f"  ã‚µãƒ–ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰æ•°: {len(subgraph.node)}")

# ã‚«ã‚¹ã‚¿ãƒ ãƒ«ãƒ¼ãƒ—æ¼”ç®—å­ã®ä¾‹
def create_loop_example():
    """ãƒ«ãƒ¼ãƒ—æ¼”ç®—å­ã®å®Ÿè£…ä¾‹"""
    # ãƒ«ãƒ¼ãƒ—æ¡ä»¶ã®ã‚°ãƒ©ãƒ•
    cond_graph = helper.make_graph(
        nodes=[],
        name="condition",
        inputs=[helper.make_tensor_value_info("i", TensorProto.INT64, [1])],
        outputs=[helper.make_tensor_value_info("cond", TensorProto.BOOL, [1])]
    )
    
    # ãƒ«ãƒ¼ãƒ—æœ¬ä½“ã®ã‚°ãƒ©ãƒ•
    body_graph = helper.make_graph(
        nodes=[],
        name="body",
        inputs=[
            helper.make_tensor_value_info("i", TensorProto.INT64, [1]),
            helper.make_tensor_value_info("sum", TensorProto.FLOAT, [1])
        ],
        outputs=[
            helper.make_tensor_value_info("cond", TensorProto.BOOL, [1]),
            helper.make_tensor_value_info("sum_out", TensorProto.FLOAT, [1])
        ]
    )
    
    print("ãƒ«ãƒ¼ãƒ—æ¼”ç®—å­ã®æ§‹é€ ã‚’ä½œæˆã—ã¾ã—ãŸ")

create_loop_example()
```

### 2.2.6ã€€æ¼”ç®—å­ã‚¹ã‚­ãƒ£ãƒ³

Scanæ¼”ç®—å­ã®è©³ç´°ä½¿ç”¨ä¾‹ï¼š

```python
import numpy as np
import onnx
from onnx import helper, TensorProto

def create_scan_example():
    """Scanæ¼”ç®—å­ã®ä½¿ç”¨ä¾‹"""
    
    # RNNãƒ©ã‚¤ã‚¯ãªæ“ä½œã‚’Scanã§å®Ÿè£…
    # å„æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§åŒã˜å¤‰æ›ã‚’é©ç”¨
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒœãƒ‡ã‚£ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    scan_body = helper.make_graph(
        nodes=[
            # å‰ã®éš ã‚ŒçŠ¶æ…‹ã¨å…¥åŠ›ã‚’åŠ ç®—
            helper.make_node(
                "Add",
                inputs=["prev_h", "x_t"],
                outputs=["h_t"],
                name="add_node"
            )
        ],
        name="scan_body",
        inputs=[
            helper.make_tensor_value_info("prev_h", TensorProto.FLOAT, [4]),  # å‰ã®çŠ¶æ…‹
            helper.make_tensor_value_info("x_t", TensorProto.FLOAT, [4])      # ç¾åœ¨ã®å…¥åŠ›
        ],
        outputs=[
            helper.make_tensor_value_info("h_t", TensorProto.FLOAT, [4])      # æ–°ã—ã„çŠ¶æ…‹
        ]
    )
    
    print("Scanæ¼”ç®—å­ã®ãƒœãƒ‡ã‚£ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")
    return scan_body

scan_body = create_scan_example()
```

### 2.2.7ã€€ãƒ„ãƒ¼ãƒ«

#### ONNXé–‹ç™ºãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆ

```python
import onnx
from onnx import checker, helper, shape_inference

def onnx_tools_demo(model_path):
    """ONNXãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ä¾‹"""
    
    # 1. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = onnx.load(model_path)
    
    # 2. ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼
    try:
        checker.check_model(model)
        print("âœ“ ãƒ¢ãƒ‡ãƒ«ã¯æœ‰åŠ¹ã§ã™")
    except Exception as e:
        print(f"âœ— ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. å½¢çŠ¶æ¨è«–
    try:
        inferred_model = shape_inference.infer_shapes(model)
        print("âœ“ å½¢çŠ¶æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
        # ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ã‚’è¡¨ç¤º
        print("ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶:")
        for value_info in inferred_model.graph.value_info:
            print(f"  {value_info.name}: {[d.dim_value for d in value_info.type.tensor_type.shape.dim]}")
            
    except Exception as e:
        print(f"âœ— å½¢çŠ¶æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 4. ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ï¼ˆç°¡å˜ãªä¾‹ï¼‰
    try:
        from onnx import optimizer
        
        # åˆ©ç”¨å¯èƒ½ãªæœ€é©åŒ–ãƒ‘ã‚¹ã‚’ç¢ºèª
        available_passes = optimizer.get_available_passes()
        print(f"åˆ©ç”¨å¯èƒ½ãªæœ€é©åŒ–ãƒ‘ã‚¹: {available_passes}")
        
        # åŸºæœ¬çš„ãªæœ€é©åŒ–ã‚’é©ç”¨
        optimized_model = optimizer.optimize(model, ['eliminate_identity'])
        print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    except ImportError:
        print("æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    except Exception as e:
        print(f"æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

# ä½¿ç”¨ä¾‹
onnx_tools_demo("model.onnx")
```

## 2.3ã€€ONNXã¨Python

### 2.3.1ã€€ç·šå½¢å›å¸°ä¾‹

ã‚·ãƒ³ãƒ—ãƒ«ãªç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ONNXã§ä½œæˆï¼š

```python
import numpy as np
import onnx
from onnx import helper, TensorProto, numpy_helper
import onnxruntime as ort

def create_linear_regression_onnx():
    """ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ONNXã§ä½œæˆ"""
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ï¼‰
    weight = np.array([[2.0, 3.0]], dtype=np.float32)  # 1x2ã®é‡ã¿è¡Œåˆ—
    bias = np.array([1.0], dtype=np.float32)           # ãƒã‚¤ã‚¢ã‚¹
    
    # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®åˆæœŸåŒ–å­ã‚’ä½œæˆ
    weight_initializer = numpy_helper.from_array(weight, name="weight")
    bias_initializer = numpy_helper.from_array(bias, name="bias")
    
    # ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
    # y = X * W^T + b
    matmul_node = helper.make_node(
        "MatMul",
        inputs=["X", "weight"],
        outputs=["matmul_result"],
        name="matmul_node"
    )
    
    add_node = helper.make_node(
        "Add",
        inputs=["matmul_result", "bias"],
        outputs=["Y"],
        name="add_node"
    )
    
    # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    graph = helper.make_graph(
        nodes=[matmul_node, add_node],
        name="LinearRegression",
        inputs=[helper.make_tensor_value_info("X", TensorProto.FLOAT, [None, 2])],
        outputs=[helper.make_tensor_value_info("Y", TensorProto.FLOAT, [None, 1])],
        initializer=[weight_initializer, bias_initializer]
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = helper.make_model(graph, producer_name="linear_regression_example")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    onnx.save(model, "linear_regression.onnx")
    print("ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: linear_regression.onnx")
    
    return model

def test_linear_regression():
    """ä½œæˆã—ãŸç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = create_linear_regression_onnx()
    
    # ONNX Runtimeã§æ¨è«–ã‚’å®Ÿè¡Œ
    session = ort.InferenceSession("linear_regression.onnx")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    X_test = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=np.float32)
    
    # æ¨è«–å®Ÿè¡Œ
    outputs = session.run(None, {"X": X_test})
    predictions = outputs[0]
    
    print("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(X_test)
    print("äºˆæ¸¬çµæœ:")
    print(predictions)
    
    # æ‰‹å‹•è¨ˆç®—ã§ã®æ¤œè¨¼
    weight = np.array([[2.0, 3.0]], dtype=np.float32)
    bias = np.array([1.0], dtype=np.float32)
    manual_pred = np.dot(X_test, weight.T) + bias
    
    print("æ‰‹å‹•è¨ˆç®—çµæœ:")
    print(manual_pred)
    
    # çµæœã®æ¯”è¼ƒ
    if np.allclose(predictions, manual_pred):
        print("âœ“ çµæœãŒä¸€è‡´ã—ã¾ã—ãŸ")
    else:
        print("âœ— çµæœãŒä¸€è‡´ã—ã¾ã›ã‚“")

# å®Ÿè¡Œ
test_linear_regression()
```

### 2.3.2ã€€åˆæœŸåŒ–å­ã€æ”¹è‰¯ã•ã‚ŒãŸç·šå½¢è¨ˆç”»æ³•

ã‚ˆã‚Šè¤‡é›‘ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¨åˆæœŸåŒ–å­ã®æ´»ç”¨ï¼š

```python
import numpy as np
import onnx
from onnx import helper, TensorProto, numpy_helper
import onnxruntime as ort

def create_advanced_linear_model():
    """æ”¹è‰¯ã•ã‚ŒãŸç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆæ­£è¦åŒ–ã€æ´»æ€§åŒ–é–¢æ•°ä»˜ãï¼‰"""
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
    input_dim = 4
    hidden_dim = 8
    output_dim = 1
    
    # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®åˆæœŸåŒ–ï¼ˆXavieråˆæœŸåŒ–ï¼‰
    W1 = np.random.randn(input_dim, hidden_dim).astype(np.float32) * np.sqrt(2.0 / input_dim)
    b1 = np.zeros(hidden_dim, dtype=np.float32)
    W2 = np.random.randn(hidden_dim, output_dim).astype(np.float32) * np.sqrt(2.0 / hidden_dim)
    b2 = np.zeros(output_dim, dtype=np.float32)
    
    # ãƒãƒƒãƒæ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    scale = np.ones(hidden_dim, dtype=np.float32)
    bias_bn = np.zeros(hidden_dim, dtype=np.float32)
    mean = np.zeros(hidden_dim, dtype=np.float32)
    var = np.ones(hidden_dim, dtype=np.float32)
    
    # åˆæœŸåŒ–å­ã®ä½œæˆ
    initializers = [
        numpy_helper.from_array(W1, name="W1"),
        numpy_helper.from_array(b1, name="b1"),
        numpy_helper.from_array(W2, name="W2"),
        numpy_helper.from_array(b2, name="b2"),
        numpy_helper.from_array(scale, name="scale"),
        numpy_helper.from_array(bias_bn, name="bias_bn"),
        numpy_helper.from_array(mean, name="mean"),
        numpy_helper.from_array(var, name="var"),
    ]
    
    # ãƒãƒ¼ãƒ‰ã®ä½œæˆ
    nodes = [
        # ç¬¬1å±¤: X @ W1 + b1
        helper.make_node("MatMul", ["X", "W1"], ["hidden1"], name="matmul1"),
        helper.make_node("Add", ["hidden1", "b1"], ["hidden1_bias"], name="add1"),
        
        # ãƒãƒƒãƒæ­£è¦åŒ–
        helper.make_node(
            "BatchNormalization",
            ["hidden1_bias", "scale", "bias_bn", "mean", "var"],
            ["bn_output"],
            epsilon=1e-5,
            name="batch_norm"
        ),
        
        # ReLUæ´»æ€§åŒ–
        helper.make_node("Relu", ["bn_output"], ["relu_output"], name="relu"),
        
        # ç¬¬2å±¤: relu_output @ W2 + b2
        helper.make_node("MatMul", ["relu_output", "W2"], ["output_matmul"], name="matmul2"),
        helper.make_node("Add", ["output_matmul", "b2"], ["Y"], name="add2"),
    ]
    
    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    graph = helper.make_graph(
        nodes=nodes,
        name="AdvancedLinearModel",
        inputs=[helper.make_tensor_value_info("X", TensorProto.FLOAT, [None, input_dim])],
        outputs=[helper.make_tensor_value_info("Y", TensorProto.FLOAT, [None, output_dim])],
        initializer=initializers
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ä¿å­˜
    model = helper.make_model(graph, producer_name="advanced_linear_model")
    onnx.save(model, "advanced_linear_model.onnx")
    
    print("æ”¹è‰¯ã•ã‚ŒãŸç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: advanced_linear_model.onnx")
    return model

def test_advanced_model():
    """æ”¹è‰¯ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = create_advanced_linear_model()
    
    # ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
    onnx.checker.check_model(model)
    print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸ")
    
    # æ¨è«–ãƒ†ã‚¹ãƒˆ
    session = ort.InferenceSession("advanced_linear_model.onnx")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    batch_size = 5
    input_dim = 4
    X_test = np.random.randn(batch_size, input_dim).astype(np.float32)
    
    # æ¨è«–å®Ÿè¡Œ
    outputs = session.run(None, {"X": X_test})
    predictions = outputs[0]
    
    print(f"å…¥åŠ›å½¢çŠ¶: {X_test.shape}")
    print(f"å‡ºåŠ›å½¢çŠ¶: {predictions.shape}")
    print(f"å‡ºåŠ›ã‚µãƒ³ãƒ—ãƒ«: {predictions[:3].flatten()}")

# å®Ÿè¡Œ
test_advanced_model()
```

### 2.3.3ã€€ONNXæ§‹é€ ã®èµ°æŸ»ã¨åˆæœŸåŒ–å­ã®ãƒã‚§ãƒƒã‚¯

```python
import onnx
from onnx import numpy_helper
import numpy as np

def traverse_onnx_structure(model_path):
    """ONNXæ§‹é€ ã‚’è©³ç´°ã«èµ°æŸ»"""
    
    model = onnx.load(model_path)
    
    print("=== ONNXæ§‹é€ ã®è©³ç´°èµ°æŸ» ===")
    
    # 1. åŸºæœ¬æƒ…å ±
    print(f"IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model.ir_version}")
    print(f"OpSetãƒãƒ¼ã‚¸ãƒ§ãƒ³: {[f'{op.domain}:{op.version}' for op in model.opset_import]}")
    
    # 2. ã‚°ãƒ©ãƒ•æƒ…å ±
    graph = model.graph
    print(f"\nã‚°ãƒ©ãƒ•å: {graph.name}")
    
    # 3. å…¥åŠ›ã®è©³ç´°åˆ†æ
    print(f"\n--- å…¥åŠ›åˆ†æ ---")
    for i, input_tensor in enumerate(graph.input):
        print(f"å…¥åŠ›{i+1}: {input_tensor.name}")
        tensor_type = input_tensor.type.tensor_type
        print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {tensor_type.elem_type}")
        
        shape_info = []
        for dim in tensor_type.shape.dim:
            if dim.dim_value:
                shape_info.append(str(dim.dim_value))
            elif dim.dim_param:
                shape_info.append(dim.dim_param)
            else:
                shape_info.append("?")
        print(f"  å½¢çŠ¶: [{', '.join(shape_info)}]")
    
    # 4. åˆæœŸåŒ–å­ã®è©³ç´°åˆ†æ
    print(f"\n--- åˆæœŸåŒ–å­åˆ†æ ---")
    total_params = 0
    
    for i, initializer in enumerate(graph.initializer):
        print(f"\nåˆæœŸåŒ–å­{i+1}: {initializer.name}")
        print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {initializer.data_type}")
        print(f"  å½¢çŠ¶: {list(initializer.dims)}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—
        param_count = np.prod(initializer.dims)
        total_params += param_count
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count}")
        
        # çµ±è¨ˆæƒ…å ±ï¼ˆå°ã•ãªãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆï¼‰
        if param_count <= 1000:
            tensor_data = numpy_helper.to_array(initializer)
            print(f"  æœ€å°å€¤: {tensor_data.min():.6f}")
            print(f"  æœ€å¤§å€¤: {tensor_data.max():.6f}")
            print(f"  å¹³å‡å€¤: {tensor_data.mean():.6f}")
            print(f"  æ¨™æº–åå·®: {tensor_data.std():.6f}")
        else:
            print(f"  (çµ±è¨ˆæƒ…å ±ã‚¹ã‚­ãƒƒãƒ—: ãƒ†ãƒ³ã‚½ãƒ«ãŒå¤§ãã™ãã¾ã™)")
    
    print(f"\nç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params}")
    
    # 5. ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
    print(f"\n--- ãƒãƒ¼ãƒ‰åˆ†æ ---")
    op_count = {}
    
    for i, node in enumerate(graph.node):
        op_type = node.op_type
        op_count[op_type] = op_count.get(op_type, 0) + 1
        
        print(f"\nãƒãƒ¼ãƒ‰{i+1}: {node.name}")
        print(f"  æ¼”ç®—å­ã‚¿ã‚¤ãƒ—: {op_type}")
        print(f"  å…¥åŠ›: {list(node.input)}")
        print(f"  å‡ºåŠ›: {list(node.output)}")
        
        # å±æ€§ã®è¡¨ç¤º
        if node.attribute:
            print(f"  å±æ€§:")
            for attr in node.attribute:
                attr_value = onnx.helper.get_attribute_value(attr)
                if isinstance(attr_value, bytes):
                    print(f"    {attr.name}: <ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿>")
                elif isinstance(attr_value, onnx.GraphProto):
                    print(f"    {attr.name}: <ã‚µãƒ–ã‚°ãƒ©ãƒ•>")
                else:
                    print(f"    {attr.name}: {attr_value}")
    
    # 6. æ¼”ç®—å­ã®çµ±è¨ˆ
    print(f"\n--- æ¼”ç®—å­çµ±è¨ˆ ---")
    for op_type, count in sorted(op_count.items()):
        print(f"{op_type}: {count}å€‹")
    
    # 7. å‡ºåŠ›ã®è©³ç´°åˆ†æ
    print(f"\n--- å‡ºåŠ›åˆ†æ ---")
    for i, output_tensor in enumerate(graph.output):
        print(f"å‡ºåŠ›{i+1}: {output_tensor.name}")
        if output_tensor.type.tensor_type:
            tensor_type = output_tensor.type.tensor_type
            print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {tensor_type.elem_type}")
            
            shape_info = []
            for dim in tensor_type.shape.dim:
                if dim.dim_value:
                    shape_info.append(str(dim.dim_value))
                elif dim.dim_param:
                    shape_info.append(dim.dim_param)
                else:
                    shape_info.append("?")
            print(f"  å½¢çŠ¶: [{', '.join(shape_info)}]")

def check_initializer_health(model_path):
    """åˆæœŸåŒ–å­ã®å¥å…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    
    model = onnx.load(model_path)
    
    print("=== åˆæœŸåŒ–å­å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ ===")
    
    issues = []
    
    for initializer in model.graph.initializer:
        tensor_data = numpy_helper.to_array(initializer)
        
        # NaNå€¤ã®ãƒã‚§ãƒƒã‚¯
        if np.isnan(tensor_data).any():
            issues.append(f"{initializer.name}: NaNå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        
        # ç„¡é™å¤§å€¤ã®ãƒã‚§ãƒƒã‚¯
        if np.isinf(tensor_data).any():
            issues.append(f"{initializer.name}: ç„¡é™å¤§å€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        
        # ç•°å¸¸ã«å¤§ããªå€¤ã®ãƒã‚§ãƒƒã‚¯
        if np.abs(tensor_data).max() > 1000:
            issues.append(f"{initializer.name}: ç•°å¸¸ã«å¤§ããªå€¤ãŒã‚ã‚Šã¾ã™ (max: {np.abs(tensor_data).max()})")
        
        # å…¨ã¦0ã®é‡ã¿ã®ãƒã‚§ãƒƒã‚¯
        if np.all(tensor_data == 0):
            issues.append(f"{initializer.name}: å…¨ã¦0ã®é‡ã¿ã§ã™")
    
    if issues:
        print("ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:")
        for issue in issues:
            print(f"  âš ï¸  {issue}")
    else:
        print("âœ“ åˆæœŸåŒ–å­ã«å•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # å…ˆã»ã©ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ†æ
    traverse_onnx_structure("advanced_linear_model.onnx")
    check_initializer_health("advanced_linear_model.onnx")
```

## 2.4ã€€æ¼”ç®—å­å±æ€§

ONNXã®æ¼”ç®—å­å±æ€§ã«ã¤ã„ã¦è©³ã—ãå­¦ç¿’ã—ã¾ã™ï¼š

```python
import onnx
from onnx import helper, TensorProto, AttributeProto
import numpy as np

def demonstrate_operator_attributes():
    """ã•ã¾ã–ã¾ãªæ¼”ç®—å­å±æ€§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("=== æ¼”ç®—å­å±æ€§ã®ãƒ‡ãƒ¢ ===")
    
    # 1. Convæ¼”ç®—å­ã®å±æ€§
    conv_node = helper.make_node(
        'Conv',
        inputs=['X', 'W'],
        outputs=['Y'],
        kernel_shape=[3, 3],      # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
        pads=[1, 1, 1, 1],       # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° [ä¸Š, å·¦, ä¸‹, å³]
        strides=[1, 1],          # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        dilations=[1, 1],        # è†¨å¼µ
        group=1,                 # ã‚°ãƒ«ãƒ¼ãƒ—ç•³ã¿è¾¼ã¿
        name="conv_example"
    )
    
    print("Convolutionæ¼”ç®—å­ã®å±æ€§:")
    for attr in conv_node.attribute:
        print(f"  {attr.name}: {helper.get_attribute_value(attr)}")
    
    # 2. BatchNormalizationæ¼”ç®—å­ã®å±æ€§
    bn_node = helper.make_node(
        'BatchNormalization',
        inputs=['X', 'scale', 'B', 'mean', 'var'],
        outputs=['Y'],
        epsilon=1e-5,            # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤
        momentum=0.9,            # ç§»å‹•å¹³å‡ã®é‡ã¿
        name="batch_norm_example"
    )
    
    print("\nBatchNormalizationæ¼”ç®—å­ã®å±æ€§:")
    for attr in bn_node.attribute:
        print(f"  {attr.name}: {helper.get_attribute_value(attr)}")
    
    # 3. Reshapeæ¼”ç®—å­ã®å±æ€§
    reshape_node = helper.make_node(
        'Reshape',
        inputs=['X', 'shape'],
        outputs=['Y'],
        allowzero=0,             # 0ã«ã‚ˆã‚‹å½¢çŠ¶æŒ‡å®šã®è¨±å¯
        name="reshape_example"
    )
    
    print("\nReshapeæ¼”ç®—å­ã®å±æ€§:")
    for attr in reshape_node.attribute:
        print(f"  {attr.name}: {helper.get_attribute_value(attr)}")
    
    return [conv_node, bn_node, reshape_node]

def create_conv_model_with_attributes():
    """å±æ€§ã‚’è©³ç´°ã«è¨­å®šã—ãŸç•³ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«"""
    
    # å…¥åŠ›: [N, C, H, W] = [1, 3, 32, 32]
    # ã‚«ãƒ¼ãƒãƒ«: [16, 3, 3, 3] (16å€‹ã®3x3ã‚«ãƒ¼ãƒãƒ«)
    
    # é‡ã¿ã®åˆæœŸåŒ–
    kernel = np.random.randn(16, 3, 3, 3).astype(np.float32) * 0.1
    bias = np.zeros(16, dtype=np.float32)
    
    # åˆæœŸåŒ–å­ã®ä½œæˆ
    kernel_initializer = helper.make_tensor(
        name="conv_kernel",
        data_type=TensorProto.FLOAT,
        dims=[16, 3, 3, 3],
        vals=kernel.flatten()
    )
    
    bias_initializer = helper.make_tensor(
        name="conv_bias",
        data_type=TensorProto.FLOAT,
        dims=[16],
        vals=bias.flatten()
    )
    
    # ãƒãƒ¼ãƒ‰ã®ä½œæˆï¼ˆè©³ç´°ãªå±æ€§è¨­å®šï¼‰
    conv_node = helper.make_node(
        'Conv',
        inputs=['input', 'conv_kernel', 'conv_bias'],
        outputs=['conv_output'],
        kernel_shape=[3, 3],         # 3x3ã‚«ãƒ¼ãƒãƒ«
        pads=[1, 1, 1, 1],          # åŒã˜ã‚µã‚¤ã‚ºå‡ºåŠ›ã®ãŸã‚ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        strides=[1, 1],             # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰1
        dilations=[1, 1],           # é€šå¸¸ã®ç•³ã¿è¾¼ã¿
        group=1,                    # é€šå¸¸ã®ç•³ã¿è¾¼ã¿ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãªã—ï¼‰
        name="detailed_conv"
    )
    
    # ReLUæ´»æ€§åŒ–
    relu_node = helper.make_node(
        'Relu',
        inputs=['conv_output'],
        outputs=['output'],
        name="relu_activation"
    )
    
    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    graph = helper.make_graph(
        nodes=[conv_node, relu_node],
        name="ConvModelWithAttributes",
        inputs=[helper.make_tensor_value_info("input", TensorProto.FLOAT, [1, 3, 32, 32])],
        outputs=[helper.make_tensor_value_info("output", TensorProto.FLOAT, [1, 16, 32, 32])],
        initializer=[kernel_initializer, bias_initializer]
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = helper.make_model(graph, producer_name="conv_attributes_demo")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    onnx.save(model, "conv_model_with_attributes.onnx")
    print("å±æ€§è©³ç´°è¨­å®šæ¸ˆã¿ç•³ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    return model

def analyze_model_attributes(model_path):
    """ãƒ¢ãƒ‡ãƒ«ã®å…¨æ¼”ç®—å­å±æ€§ã‚’åˆ†æ"""
    
    model = onnx.load(model_path)
    
    print(f"=== {model_path} ã®å±æ€§åˆ†æ ===")
    
    for i, node in enumerate(model.graph.node):
        print(f"\nãƒãƒ¼ãƒ‰ {i+1}: {node.name} ({node.op_type})")
        
        if node.attribute:
            print("  å±æ€§:")
            for attr in node.attribute:
                attr_value = helper.get_attribute_value(attr)
                
                # å±æ€§ã‚¿ã‚¤ãƒ—ã”ã¨ã®å‡¦ç†
                if attr.type == AttributeProto.INT:
                    print(f"    {attr.name} (INT): {attr_value}")
                elif attr.type == AttributeProto.INTS:
                    print(f"    {attr.name} (INTS): {list(attr_value)}")
                elif attr.type == AttributeProto.FLOAT:
                    print(f"    {attr.name} (FLOAT): {attr_value}")
                elif attr.type == AttributeProto.FLOATS:
                    print(f"    {attr.name} (FLOATS): {list(attr_value)}")
                elif attr.type == AttributeProto.STRING:
                    print(f"    {attr.name} (STRING): {attr_value.decode()}")
                elif attr.type == AttributeProto.TENSOR:
                    print(f"    {attr.name} (TENSOR): å½¢çŠ¶{attr_value.dims}")
                elif attr.type == AttributeProto.GRAPH:
                    print(f"    {attr.name} (GRAPH): ãƒãƒ¼ãƒ‰æ•°{len(attr_value.node)}")
                else:
                    print(f"    {attr.name}: {attr_value}")
        else:
            print("  å±æ€§: ãªã—")

# ãƒ‡ãƒ¢ã®å®Ÿè¡Œ
if __name__ == "__main__":
    # åŸºæœ¬çš„ãªå±æ€§ãƒ‡ãƒ¢
    demonstrate_operator_attributes()
    
    # è©³ç´°ãªç•³ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    create_conv_model_with_attributes()
    
    # å±æ€§ã®åˆ†æ
    analyze_model_attributes("conv_model_with_attributes.onnx")
```

## 2.5ã€€å®Ÿè·µçš„ãªé–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 2.5.1ã€€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°

å®Ÿéš›ã®é–‹ç™ºã§ã¯ã€ã•ã¾ã–ã¾ãªã‚¨ãƒ©ãƒ¼ã«é­é‡ã—ã¾ã™ã€‚åŠ¹æœçš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’æ•´ç†ã—ã¾ã™ã€‚

```python
import onnxruntime as ort
import numpy as np
import logging
from typing import Dict, Any, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ONNXInferenceEngine:
    """å …ç‰¢ãªONNXæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_path: str, providers: Optional[list] = None):
        """
        ONNXãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            providers: ä½¿ç”¨ã™ã‚‹å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        """
        self.model_path = model_path
        self.session = None
        self.input_names = []
        self.output_names = []
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if providers is None:
            providers = self._get_optimal_providers()
        
        try:
            self._initialize_session(providers)
            self._analyze_model_io()
            logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ: {model_path}")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            raise
    
    def _get_optimal_providers(self) -> list:
        """æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•é¸æŠ"""
        available = ort.get_available_providers()
        
        # å„ªå…ˆé †ä½ã«åŸºã¥ã„ãŸé¸æŠ
        preferred_order = [
            'TensorrtExecutionProvider',
            'CUDAExecutionProvider', 
            'OpenVINOExecutionProvider',
            'DmlExecutionProvider',
            'CPUExecutionProvider'
        ]
        
        selected = []
        for provider in preferred_order:
            if provider in available:
                selected.append(provider)
        
        logger.info(f"é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {selected}")
        return selected
    
    def _initialize_session(self, providers: list):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        session_options = ort.SessionOptions()
        
        # æœ€é©åŒ–è¨­å®š
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        session_options.enable_cpu_mem_arena = True
        session_options.enable_mem_pattern = True
        
        # ãƒ­ã‚°è¨­å®š
        session_options.log_severity_level = 3  # ERRORä»¥ä¸Šã®ã¿
        
        try:
            self.session = ort.InferenceSession(
                self.model_path,
                session_options,
                providers=providers
            )
        except Exception as e:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–¢é€£ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if any(keyword in str(e).lower() for keyword in ['provider', 'cuda', 'gpu']):
                logger.warning(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼ã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                self.session = ort.InferenceSession(
                    self.model_path,
                    session_options,
                    providers=['CPUExecutionProvider']
                )
            else:
                raise
    
    def _analyze_model_io(self):
        """ãƒ¢ãƒ‡ãƒ«ã®å…¥å‡ºåŠ›ã‚’åˆ†æ"""
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        # å…¥åŠ›è©³ç´°ã®è¨˜éŒ²
        for input_meta in self.session.get_inputs():
            logger.info(f"å…¥åŠ› '{input_meta.name}': å½¢çŠ¶={input_meta.shape}, å‹={input_meta.type}")
        
        # å‡ºåŠ›è©³ç´°ã®è¨˜éŒ²
        for output_meta in self.session.get_outputs():
            logger.info(f"å‡ºåŠ› '{output_meta.name}': å½¢çŠ¶={output_meta.shape}, å‹={output_meta.type}")
    
    def predict(self, inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        äºˆæ¸¬å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            inputs: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
            
        Returns:
            å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        try:
            # å…¥åŠ›æ¤œè¨¼
            self._validate_inputs(inputs)
            
            # æ¨è«–å®Ÿè¡Œ
            outputs = self.session.run(self.output_names, inputs)
            
            # å‡ºåŠ›ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™
            return {name: output for name, output in zip(self.output_names, outputs)}
            
        except Exception as e:
            logger.error(f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _validate_inputs(self, inputs: Dict[str, np.ndarray]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        # å¿…è¦ãªå…¥åŠ›ãŒã™ã¹ã¦æä¾›ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        for required_input in self.input_names:
            if required_input not in inputs:
                raise ValueError(f"å¿…è¦ãªå…¥åŠ›ãŒä¸è¶³: {required_input}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã¨å½¢çŠ¶ã®æ¤œè¨¼
        for input_meta in self.session.get_inputs():
            input_name = input_meta.name
            expected_shape = input_meta.shape
            actual_data = inputs[input_name]
            
            # NaNå€¤ã®ãƒã‚§ãƒƒã‚¯
            if np.isnan(actual_data).any():
                raise ValueError(f"å…¥åŠ›'{input_name}'ã«NaNå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            
            # ç„¡é™å¤§å€¤ã®ãƒã‚§ãƒƒã‚¯
            if np.isinf(actual_data).any():
                raise ValueError(f"å…¥åŠ›'{input_name}'ã«ç„¡é™å¤§å€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            
            # å½¢çŠ¶ã®å‹•çš„ãƒã‚§ãƒƒã‚¯ï¼ˆ-1ã¯å‹•çš„æ¬¡å…ƒã‚’è¡¨ã™ï¼‰
            if len(expected_shape) != len(actual_data.shape):
                raise ValueError(
                    f"å…¥åŠ›'{input_name}'ã®æ¬¡å…ƒæ•°ãŒä¸ä¸€è‡´: "
                    f"æœŸå¾…={len(expected_shape)}, å®Ÿéš›={len(actual_data.shape)}"
                )

# ä½¿ç”¨ä¾‹
def test_robust_inference():
    """å …ç‰¢ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    try:
        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        engine = ONNXInferenceEngine("model.onnx")
        
        # ãƒ†ã‚¹ãƒˆç”¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        test_input = np.random.randn(1, 224, 224, 3).astype(np.float32)
        inputs = {"input": test_input}
        
        # æ¨è«–å®Ÿè¡Œ
        results = engine.predict(inputs)
        
        logger.info(f"æ¨è«–æˆåŠŸ: å‡ºåŠ›ã‚­ãƒ¼={list(results.keys())}")
        
    except FileNotFoundError:
        logger.error("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
```

### 2.5.2ã€€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼š

```python
import onnxruntime as ort
import numpy as np
import time
from contextlib import contextmanager

class PerformanceOptimizer:
    """ONNXæ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«"""
    
    @staticmethod
    def create_optimized_session(model_path: str, optimization_level: str = "all") -> ort.InferenceSession:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        
        session_options = ort.SessionOptions()
        
        # æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«è¨­å®š
        optimization_map = {
            "basic": ort.GraphOptimizationLevel.ORT_ENABLE_BASIC,
            "extended": ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED, 
            "all": ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        }
        session_options.graph_optimization_level = optimization_map[optimization_level]
        
        # ä¸¦åˆ—å®Ÿè¡Œè¨­å®š
        session_options.intra_op_num_threads = 0  # ã‚·ã‚¹ãƒ†ãƒ æœ€é©å€¤ã‚’ä½¿ç”¨
        session_options.inter_op_num_threads = 0  # ã‚·ã‚¹ãƒ†ãƒ æœ€é©å€¤ã‚’ä½¿ç”¨
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        session_options.enable_mem_pattern = True
        session_options.enable_cpu_mem_arena = True
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰æœ€é©åŒ–
        session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
        
        return ort.InferenceSession(model_path, session_options)
    
    @contextmanager
    def benchmark_context(self, name: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        start_time = time.perf_counter()
        try:
            yield
        finally:
            end_time = time.perf_counter()
            print(f"{name}: {(end_time - start_time) * 1000:.2f} ms")
    
    def benchmark_inference(self, session: ort.InferenceSession, inputs: dict, num_runs: int = 100):
        """æ¨è«–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"=== æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ({num_runs}å›å®Ÿè¡Œ) ===")
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç­‰ã®ãŸã‚ï¼‰
        for _ in range(10):
            session.run(None, inputs)
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        times = []
        for _ in range(num_runs):
            start = time.perf_counter()
            session.run(None, inputs)
            end = time.perf_counter()
            times.append((end - start) * 1000)  # ãƒŸãƒªç§’ã«å¤‰æ›
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        times = np.array(times)
        print(f"å¹³å‡å®Ÿè¡Œæ™‚é–“: {times.mean():.2f} ms")
        print(f"æ¨™æº–åå·®: {times.std():.2f} ms")
        print(f"æœ€å°å®Ÿè¡Œæ™‚é–“: {times.min():.2f} ms")
        print(f"æœ€å¤§å®Ÿè¡Œæ™‚é–“: {times.max():.2f} ms")
        print(f"95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {np.percentile(times, 95):.2f} ms")
        
        return times

# ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ã®ä¾‹
def optimize_batch_processing(model_path: str, data: np.ndarray, batch_size: int = 32):
    """ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹æœ€é©åŒ–"""
    
    session = PerformanceOptimizer.create_optimized_session(model_path)
    optimizer = PerformanceOptimizer()
    
    total_samples = data.shape[0]
    num_batches = (total_samples + batch_size - 1) // batch_size
    
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º{batch_size}ã§{num_batches}ãƒãƒƒãƒã‚’å‡¦ç†")
    
    with optimizer.benchmark_context("ãƒãƒƒãƒå‡¦ç†"):
        results = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_samples)
            batch_data = data[start_idx:end_idx]
            
            outputs = session.run(None, {"input": batch_data})
            results.append(outputs[0])
    
    return np.concatenate(results, axis=0)
```

### 2.5.3ã€€ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

```python
import os
import json
import hashlib
from pathlib import Path
from typing import Dict, Any
import onnx

class ONNXModelManager:
    """ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†"""
    
    def __init__(self, model_registry_path: str = "./model_registry"):
        self.registry_path = Path(model_registry_path)
        self.registry_path.mkdir(exist_ok=True)
        self.models_path = self.registry_path / "models"
        self.models_path.mkdir(exist_ok=True)
        self.metadata_file = self.registry_path / "metadata.json"
        self._load_metadata()
    
    def _load_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {}
    
    def _save_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
    
    def register_model(self, model_path: str, model_name: str, version: str, 
                      description: str = "", tags: list = None) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²"""
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        model_hash = self._calculate_file_hash(model_path)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
        model_info = self._extract_model_info(model_path)
        
        # ä¿å­˜ãƒ‘ã‚¹è¨­å®š
        model_filename = f"{model_name}_{version}.onnx"
        destination_path = self.models_path / model_filename
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼
        import shutil
        shutil.copy2(model_path, destination_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
        model_key = f"{model_name}:{version}"
        self.metadata[model_key] = {
            "name": model_name,
            "version": version,
            "description": description,
            "tags": tags or [],
            "file_path": str(destination_path),
            "file_hash": model_hash,
            "model_info": model_info,
            "registered_at": time.time()
        }
        
        self._save_metadata()
        print(f"ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†: {model_key}")
        return model_key
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def _extract_model_info(self, model_path: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        model = onnx.load(model_path)
        
        return {
            "ir_version": model.ir_version,
            "producer_name": model.producer_name,
            "producer_version": model.producer_version,
            "opset_versions": [{"domain": op.domain, "version": op.version} 
                             for op in model.opset_import],
            "inputs": [{"name": inp.name, "shape": str([d.dim_value for d in inp.type.tensor_type.shape.dim])} 
                      for inp in model.graph.input],
            "outputs": [{"name": out.name, "shape": str([d.dim_value for d in out.type.tensor_type.shape.dim])} 
                       for out in model.graph.output],
            "num_parameters": sum(np.prod(init.dims) for init in model.graph.initializer)
        }
    
    def list_models(self, name_filter: str = None) -> Dict[str, Any]:
        """ç™»éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§è¡¨ç¤º"""
        filtered_models = {}
        
        for key, info in self.metadata.items():
            if name_filter is None or name_filter in info["name"]:
                filtered_models[key] = info
        
        return filtered_models
    
    def load_model(self, model_name: str, version: str = "latest") -> str:
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¹å–å¾—ï¼‰"""
        if version == "latest":
            # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ¤œç´¢
            matching_keys = [k for k in self.metadata.keys() if k.startswith(f"{model_name}:")]
            if not matching_keys:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆç°¡å˜ãªæ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆï¼‰
            latest_key = sorted(matching_keys)[-1]
        else:
            latest_key = f"{model_name}:{version}"
            if latest_key not in self.metadata:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ« '{latest_key}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        model_info = self.metadata[latest_key]
        model_path = model_info["file_path"]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§ã®ç¢ºèª
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        current_hash = self._calculate_file_hash(model_path)
        if current_hash != model_info["file_hash"]:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {latest_key}")
        return model_path

# ä½¿ç”¨ä¾‹
def model_management_demo():
    """ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ‡ãƒ¢"""
    manager = ONNXModelManager("./my_model_registry")
    
    # ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
    try:
        manager.register_model(
            "my_model.onnx", 
            "image_classifier", 
            "v1.0",
            "ç”»åƒåˆ†é¡ç”¨ã®ResNetãƒ¢ãƒ‡ãƒ«",
            ["classification", "resnet", "imagenet"]
        )
    except FileNotFoundError:
        print("ãƒ‡ãƒ¢ç”¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    models = manager.list_models()
    print("ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«:")
    for key, info in models.items():
        print(f"  {key}: {info['description']}")
```

## ã¾ã¨ã‚

ç¬¬2ç« ã§ã¯ã€ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®è©³ç´°ãªæ©Ÿèƒ½ã¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºæŠ€è¡“ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«å­¦ç¿’ã—ã¾ã—ãŸï¼š

### å­¦ç¿’å†…å®¹ã®è¦ç´„

1. **å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è©³ç´°ç†è§£**
   - CPUã€CUDAã€TensorRTã€OpenVINOç­‰ã®ç‰¹æ€§ã¨é¸æŠæ–¹æ³•
   - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®æœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
   - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ã®æ´»ç”¨

2. **ONNXåŸºæœ¬æ¦‚å¿µã®å®Ÿè£…**
   - ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®è©³ç´°åˆ†æï¼ˆãƒãƒ¼ãƒ‰ã€åˆæœŸåŒ–å­ã€å±æ€§ï¼‰
   - ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨Opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç®¡ç†
   - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æ¼”ç®—å­ï¼ˆLoopã€Scanï¼‰ã®å®Ÿè£…

3. **Pythonçµ±åˆã«ã‚ˆã‚‹å®Ÿè·µçš„é–‹ç™º**
   - ç·šå½¢å›å¸°ã‹ã‚‰CNNã¾ã§ã®ãƒ¢ãƒ‡ãƒ«ä½œæˆ
   - åˆæœŸåŒ–å­ã®è©³ç´°ç®¡ç†ã¨å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
   - å½¢çŠ¶æ¨è«–ã¨æœ€é©åŒ–ãƒ‘ã‚¹ã®æ´»ç”¨

4. **æ¼”ç®—å­å±æ€§ã®å®Œå…¨ç†è§£**
   - Convolutionã€BatchNormalizationç­‰ã®è©³ç´°è¨­å®š
   - å±æ€§ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†æã¨åˆ¶å¾¡æ–¹æ³•
   - ã‚«ã‚¹ã‚¿ãƒ å±æ€§ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

5. **å®Ÿè·µçš„é–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¿’å¾—**
   - å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
   - ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

### é–‹ç™ºè€…ã¸ã®å®Ÿè·µçš„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

ã“ã®ç« ã§å­¦ã‚“ã æŠ€è¡“ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®èƒ½åŠ›ã‚’ç²å¾—ã—ã¾ã—ãŸï¼š

- **ç”£æ¥­ãƒ¬ãƒ™ãƒ«ã®æ¨è«–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°æ©Ÿèƒ½ã‚’å‚™ãˆãŸå …ç‰¢ãªã‚·ã‚¹ãƒ†ãƒ 
- **æœ€é©åŒ–æˆ¦ç•¥ã®å®Ÿè£…**: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç‰¹æ€§ã«å¿œã˜ãŸæœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
- **åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ç®¡ç†**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ 
- **è©³ç´°ãªãƒ‡ãƒãƒƒã‚°èƒ½åŠ›**: ONNXæ§‹é€ ã®å®Œå…¨ãªåˆ†æã¨å•é¡Œç‰¹å®š

æ¬¡ç« ã§ã¯ã€ã“ã‚Œã‚‰ã®åŸºç¤æŠ€è¡“ã‚’ç™ºå±•ã•ã›ã€ONNXã®é«˜åº¦ãªæ©Ÿèƒ½ã¨æ€§èƒ½åˆ†ææ‰‹æ³•ã«ã¤ã„ã¦å­¦ç¿’ã—ã¦ã„ãã¾ã™ã€‚å®Ÿéš›ã®å•†ç”¨ç’°å¢ƒã§æ±‚ã‚ã‚‰ã‚Œã‚‹ã€ã‚ˆã‚Šè¤‡é›‘ã§é«˜æ€§èƒ½ãªã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
