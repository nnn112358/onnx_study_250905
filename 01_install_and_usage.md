# ç¬¬1ç«  ONNXã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ä½¿ç”¨

> æœ¬ç« ã®æ¦‚è¦: ONNX Runtimeï¼ˆORTï¼‰ã®æ¦‚è¦ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€Python/C#/C++ã§ã®åŸºæœ¬ä½¿ç”¨ã€ãã—ã¦ä¸€éƒ¨ã®é«˜åº¦ãªAPIæ´»ç”¨ã¾ã§ã‚’æ®µéšçš„ã«è§£èª¬ã—ã¾ã™ã€‚å¿…è¦ãªã‚³ãƒãƒ³ãƒ‰ã¨æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã«çµã‚Šã€å®Ÿå‹™ã«ç›´çµã™ã‚‹æ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚

ONNXï¼ˆOpen Neural Network Exchangeï¼‰ã¯ã€ç•°ãªã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã®æ¨™æº–åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚ONNX Runtimeã¯ã€ONNXãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

## 1.1 ONNX Runtimeï¼ˆORTï¼‰ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ONNX Runtimeã¯ã€é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’æ¨è«–ã‚’å®Ÿç¾ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ã•ã¾ã–ã¾ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ï¼ˆCPU/GPU/å°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æœ¬ç•ªç’°å¢ƒã§ã®å±•é–‹ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚

### 1.1.1 ç’°å¢ƒè¦ä»¶

ONNX Runtimeã‚’æ­£ã—ãå‹•ä½œã•ã›ã‚‹ã«ã¯ã€é©åˆ‡ãªã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒãŒå¿…è¦ã§ã™ã€‚ä»¥ä¸‹ã‚’äº‹å‰ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚

**ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶:**
- **Python**: 3.7ä»¥ä¸Šï¼ˆPythonä½¿ç”¨ã®å ´åˆï¼‰
- **OS**: Windows 10ã€macOS 10.15ä»¥ä¸Šã€ã¾ãŸã¯Linux
- **ãƒ¡ãƒ¢ãƒª**: æœ€ä½4GB RAMï¼ˆæ¨å¥¨8GBä»¥ä¸Šï¼‰
- **ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡**: æœ€ä½1GB

**GPUä½¿ç”¨æ™‚ã®è¿½åŠ è¦ä»¶:**
- **NVIDIA GPU**: CUDA Compute Capability 6.0ä»¥ä¸Š
- **CUDA**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³11.6ä»¥ä¸Š
- **cuDNN**: å¯¾å¿œã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³

ã“ã‚Œã‚‰ã‚’æº€ãŸã™ã¨ã€ONNX Runtimeã®æ©Ÿèƒ½ã‚’æœ€å¤§é™ã«æ´»ç”¨ã§ãã¾ã™ã€‚ç‰¹ã«GPUã‚’ä½¿ã†å ´åˆã¯ã€CUDAç’°å¢ƒã®æ­£ã—ã„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒé‡è¦ã§ã™ã€‚

### 1.1.2 Pythonã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ONNX Runtimeã¯å……å®Ÿã—ãŸPython APIã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã¯Pythonç’°å¢ƒã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã§ã™ã€‚

#### pipã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ONNX Runtimeã¯PyPIï¼ˆPython Package Indexï¼‰ã§é…å¸ƒã•ã‚Œã¦ã„ã¾ã™ã€‚`pip`ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

**CPUç‰ˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**
```bash
# CPUç‰ˆã®ONNX Runtimeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install onnxruntime
```

CPUç‰ˆã¯æœ€ã‚‚è»½é‡ã§äº’æ›æ€§ãŒé«˜ãã€ã»ã¨ã‚“ã©ã®ç’°å¢ƒã§å‹•ä½œã—ã¾ã™ã€‚ç‰¹åˆ¥ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶ãŒãªãã€åˆå­¦è€…ã«ãŠã™ã™ã‚ã§ã™ã€‚

**GPUç‰ˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**
```bash
# GPUç‰ˆã®ONNX Runtimeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆCUDAå¯¾å¿œï¼‰
pip install onnxruntime-gpu
```

GPUç‰ˆã¯ã€NVIDIA GPUã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¨è«–ãŒå¯èƒ½ã§ã™ãŒã€CUDAç’°å¢ƒãŒæ­£ã—ãã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ONNXã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**
```bash
# ONNXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆãƒ»ç·¨é›†ã«å¿…è¦ï¼‰
pip install onnx
```

ONNXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€ONNXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ»ä½œæˆãƒ»å¤‰æ›´ã‚’è¡Œã†ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ONNX Runtimeã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€é–‹ç™ºç’°å¢ƒãŒæ•´ã„ã¾ã™ã€‚

> æ³¨æ„: CPUç‰ˆã¨GPUç‰ˆã¯åŒæ™‚ã«å…±å­˜ã§ãã¾ã›ã‚“ã€‚GPUç‰ˆã‚’å…¥ã‚Œã‚‹å ´åˆã¯ã€å…ˆã«CPUç‰ˆã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆ`pip uninstall onnxruntime`ï¼‰ã—ã¦ãã ã•ã„ã€‚

#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ç¢ºèª

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®æˆå¦ã¨ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆã‚’ä»¥ä¸‹ã§ç¢ºèªã—ã¾ã™ã€‚

```python
import onnxruntime as ort
import onnx

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º
print(f"ONNX Runtime ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {ort.__version__}")
print(f"ONNXãƒãƒ¼ã‚¸ãƒ§ãƒ³: {onnx.__version__}")

# åˆ©ç”¨å¯èƒ½ãªå®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèª
providers = ort.get_available_providers()
print(f"åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers}")

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è©³ç´°è¡¨ç¤º
print("\n=== ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆè©³ç´° ===")
for provider in providers:
    print(f"âœ“ {provider}")

# GPUå¯¾å¿œã®ç¢ºèª
if 'CUDAExecutionProvider' in providers:
    print("\nğŸš€ GPUï¼ˆCUDAï¼‰ã‚µãƒãƒ¼ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™")
else:
    print("\nğŸ’» CPUæ¨è«–ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™")
```

å®Ÿè¡Œä¾‹:

```
ONNX Runtime ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.16.3
ONNXãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.15.0
åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ['CUDAExecutionProvider', 'CPUExecutionProvider']

=== ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆè©³ç´° ===
âœ“ CUDAExecutionProvider
âœ“ CPUExecutionProvider

ğŸš€ GPUï¼ˆCUDAï¼‰ã‚µãƒãƒ¼ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™
```

**å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®èª¬æ˜:**
- `CPUExecutionProvider`: CPUä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
- `CUDAExecutionProvider`: NVIDIA GPUä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ  
- `DmlExecutionProvider`: DirectMLï¼ˆWindows GPUï¼‰ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
- ãã®ä»–ï¼šTensorRTã€OpenVINOç­‰ã®å°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿å¯¾å¿œ

### 1.1.3 C#/C/C++/WinMLã§ã®å°å…¥

ONNX Runtimeã¯ã€Pythonä»¥å¤–ã«ã‚‚.NETï¼ˆC#ï¼‰ã‚„C++ãªã©ã‹ã‚‰åˆ©ç”¨ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã•ã¾ã–ã¾ãªé–‹ç™ºç’°å¢ƒã‚„æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã§ONNXãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚

#### C#ã§ã®ä½¿ç”¨

C#ï¼ˆ.NETï¼‰ã¯ã€ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãŒé–‹ç™ºã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã€Windowsã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ä¼æ¥­ã‚·ã‚¹ãƒ†ãƒ ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ONNX Runtimeã®C# APIã¯ã€.NET FrameworkãŠã‚ˆã³.NET Coreã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

**NuGetãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**

NuGetï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚²ãƒƒãƒˆï¼‰ã¯ã€.NETç”¨ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚Visual Studioã®Package Manager Consoleã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š

```bash
# CPUç‰ˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
Install-Package Microsoft.ML.OnnxRuntime

# GPUç‰ˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆNVIDIA CUDAå¯¾å¿œï¼‰
Install-Package Microsoft.ML.OnnxRuntime.Gpu

# ç‰¹å®šã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆ
Install-Package Microsoft.ML.OnnxRuntime -Version 1.16.3
```

**åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹:**

```csharp
using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;
using System;

class Program
{
    static void Main()
    {
        // ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        string modelPath = "model.onnx";
        
        // æ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
        using var session = new InferenceSession(modelPath);
        
        // ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
        Console.WriteLine("=== ãƒ¢ãƒ‡ãƒ«æƒ…å ± ===");
        Console.WriteLine($"å…¥åŠ›æ•°: {session.InputMetadata.Count}");
        Console.WriteLine($"å‡ºåŠ›æ•°: {session.OutputMetadata.Count}");
        
        // å…¥åŠ›æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        foreach (var input in session.InputMetadata)
        {
            Console.WriteLine($"å…¥åŠ›å: {input.Key}");
            Console.WriteLine($"ãƒ‡ãƒ¼ã‚¿å‹: {input.Value.ElementType}");
            Console.WriteLine($"å½¢çŠ¶: [{string.Join(", ", input.Value.Dimensions)}]");
        }
        
        // å‡ºåŠ›æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        foreach (var output in session.OutputMetadata)
        {
            Console.WriteLine($"å‡ºåŠ›å: {output.Key}");
            Console.WriteLine($"ãƒ‡ãƒ¼ã‚¿å‹: {output.Value.ElementType}");
            Console.WriteLine($"å½¢çŠ¶: [{string.Join(", ", output.Value.Dimensions)}]");
        }
    }
}
```

**ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š:**

```csharp
// ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è©³ç´°è¨­å®š
var options = new SessionOptions();
options.GraphOptimizationLevel = GraphOptimizationLevel.ORT_ENABLE_ALL;
options.IntraOpNumThreads = 4;  // ä¸¦åˆ—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

// GPUä½¿ç”¨ã®æŒ‡å®š
options.AppendExecutionProvider_CUDA(0);  // GPU ID 0ã‚’ä½¿ç”¨

using var session = new InferenceSession(modelPath, options);
```

#### C++ã§ã®ä½¿ç”¨

C++ã¯ã€é«˜æ€§èƒ½ãŒè¦æ±‚ã•ã‚Œã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ONNX Runtimeã®C++ APIã¯ã€æœ€å°é™ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§é«˜é€Ÿãªæ¨è«–ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

**ç’°å¢ƒæ§‹ç¯‰:**

C++ã§ONNX Runtimeã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚å…¬å¼ãƒªãƒªãƒ¼ã‚¹ã®äº‹å‰ãƒ“ãƒ«ãƒ‰ã‚’åˆ©ç”¨ã™ã‚‹ã‹ã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã—ã¾ã™ã€‚

1. [ONNX Runtime ãƒªãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¸](https://github.com/microsoft/onnxruntime/releases)ã‹ã‚‰å¯¾å¿œãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
2. ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ‰ãƒ‘ã‚¹ã¨ãƒªãƒ³ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¨­å®š
3. å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ï¼š`onnxruntime_cxx_api.h`

**åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹:**

```cpp
#include <onnxruntime_cxx_api.h>
#include <iostream>
#include <vector>
#include <string>

int main() {
    try {
        // ONNX Runtimeç’°å¢ƒã®åˆæœŸåŒ–
        // ç¬¬1å¼•æ•°ï¼šãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ï¼ˆWARNING, INFO, ERRORç­‰ï¼‰
        // ç¬¬2å¼•æ•°ï¼šãƒ­ã‚°è­˜åˆ¥ç”¨ã®åå‰
        Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "ONNXTutorial");
        
        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š
        Ort::SessionOptions session_options;
        session_options.SetIntraOpNumThreads(4);  // ä¸¦åˆ—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
        session_options.SetGraphOptimizationLevel(
            GraphOptimizationLevel::ORT_ENABLE_ALL
        );
        
        // ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        const char* model_path = "model.onnx";
        // Windowsã®å ´åˆï¼šconst wchar_t* model_path = L"model.onnx";
        Ort::Session session(env, model_path, session_options);
        
        // ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®æƒ…å ±å–å¾—
        size_t num_input_nodes = session.GetInputCount();
        size_t num_output_nodes = session.GetOutputCount();
        
        std::cout << "=== ãƒ¢ãƒ‡ãƒ«æƒ…å ± ===" << std::endl;
        std::cout << "å…¥åŠ›æ•°: " << num_input_nodes << std::endl;
        std::cout << "å‡ºåŠ›æ•°: " << num_output_nodes << std::endl;
        
        // å…¥åŠ›è©³ç´°æƒ…å ±ã®å–å¾—
        for (size_t i = 0; i < num_input_nodes; i++) {
            // å…¥åŠ›åã®å–å¾—
            Ort::AllocatorWithDefaultOptions allocator;
            auto input_name = session.GetInputNameAllocated(i, allocator);
            
            // å…¥åŠ›å‹æƒ…å ±ã®å–å¾—
            auto input_type_info = session.GetInputTypeInfo(i);
            auto tensor_info = input_type_info.GetTensorTypeAndShapeInfo();
            
            auto input_shape = tensor_info.GetShape();
            
            std::cout << "å…¥åŠ› " << i << ": " << input_name.get() << std::endl;
            std::cout << "  å½¢çŠ¶: [";
            for (size_t j = 0; j < input_shape.size(); j++) {
                std::cout << input_shape[j];
                if (j < input_shape.size() - 1) std::cout << ", ";
            }
            std::cout << "]" << std::endl;
        }
        
        // å‡ºåŠ›è©³ç´°æƒ…å ±ã®å–å¾—
        for (size_t i = 0; i < num_output_nodes; i++) {
            Ort::AllocatorWithDefaultOptions allocator;
            auto output_name = session.GetOutputNameAllocated(i, allocator);
            
            std::cout << "å‡ºåŠ› " << i << ": " << output_name.get() << std::endl;
        }
        
    } catch (const Ort::Exception& e) {
        std::cerr << "ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼: " << e.what() << std::endl;
        return -1;
    }
    
    std::cout << "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚" << std::endl;
    return 0;
}
```

**CMakeã‚’ä½¿ç”¨ã—ãŸãƒ“ãƒ«ãƒ‰è¨­å®šä¾‹:**

```cmake
cmake_minimum_required(VERSION 3.15)
project(ONNXTutorial)

set(CMAKE_CXX_STANDARD 17)

# ONNX Runtimeã®ãƒ‘ã‚¹ã‚’è¨­å®š
set(ONNXRUNTIME_ROOT_PATH "/path/to/onnxruntime")
set(ONNXRUNTIME_INCLUDE_DIRS "${ONNXRUNTIME_ROOT_PATH}/include")
set(ONNXRUNTIME_LIB_DIRS "${ONNXRUNTIME_ROOT_PATH}/lib")

# ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
include_directories(${ONNXRUNTIME_INCLUDE_DIRS})

# å®Ÿè¡Œå¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
add_executable(onnx_tutorial main.cpp)

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒªãƒ³ã‚¯
target_link_directories(onnx_tutorial PRIVATE ${ONNXRUNTIME_LIB_DIRS})
target_link_libraries(onnx_tutorial onnxruntime)
```

## 1.2 ONNX Runtimeã®ä½¿ç”¨

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ãŸã‚‰ã€å®Ÿéš›ã«ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚ã“ã®ç¯€ã§ã¯ã€Pythonã¨C++ã§ã®å…·ä½“çš„ãªæ¨è«–å®Ÿè¡Œæ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

### 1.2.1 Pythonã§ã®ONNX Runtimeä½¿ç”¨

Pythonã§ã®æ¨è«–å®Ÿè¡Œã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®ç ”ç©¶é–‹ç™ºã«ãŠã„ã¦æœ€ã‚‚ä¸€èˆ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚Pythonã®NumPyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã®è¦ªå’Œæ€§ãŒé«˜ãã€ãƒ‡ãƒ¼ã‚¿ã®å‰å¾Œå‡¦ç†ã‚‚å®¹æ˜“ã«è¡Œãˆã¾ã™ã€‚

#### åŸºæœ¬çš„ãªæ¨è«–ã®å®Ÿè¡Œ

ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¨è«–ã®åŸºæœ¬çš„ãªæµã‚Œã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

1. **ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿**: ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
2. **å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™**: ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
3. **æ¨è«–ã®å®Ÿè¡Œ**: æº–å‚™ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œ
4. **çµæœã®å–å¾—**: æ¨è«–çµæœã‚’å–å¾—ãƒ»å‡¦ç†

```python
import onnxruntime as ort
import numpy as np

# Step 1: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
print("ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
session = ort.InferenceSession("model.onnx")
print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

# Step 2: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
# ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ä»•æ§˜ã‚’å–å¾—
input_name = session.get_inputs()[0].name
input_shape = session.get_inputs()[0].shape
input_type = session.get_inputs()[0].type

print(f"å…¥åŠ›ä»•æ§˜:")
print(f"  åå‰: {input_name}")
print(f"  å½¢çŠ¶: {input_shape}")
print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {input_type}")

# å‹•çš„æ¬¡å…ƒï¼ˆ-1ã‚„Noneï¼‰ã®å‡¦ç†
# ä¾‹ï¼š[None, 3, 224, 224] â†’ [1, 3, 224, 224]
actual_input_shape = []
for dim in input_shape:
    if dim is None or dim == -1:
        actual_input_shape.append(1)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«è¨­å®š
    else:
        actual_input_shape.append(dim)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã§å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
input_data = np.random.randn(*actual_input_shape).astype(np.float32)
print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {input_data.shape}")

# Step 3: æ¨è«–ã®å®Ÿè¡Œ
print("æ¨è«–ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...")
outputs = session.run(None, {input_name: input_data})
print("âœ“ æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸ")

# Step 4: çµæœã®å–å¾—ã¨è¡¨ç¤º
print(f"\n=== æ¨è«–çµæœ ===")
for i, output in enumerate(outputs):
    print(f"å‡ºåŠ›{i+1}:")
    print(f"  å½¢çŠ¶: {output.shape}")
    print(f"  ãƒ‡ãƒ¼ã‚¿å‹: {output.dtype}")
    print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{output.min():.4f}, {output.max():.4f}]")
    
    # å°ã•ãªãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆã¯å®Ÿéš›ã®å€¤ã‚‚è¡¨ç¤º
    if output.size <= 10:
        print(f"  å€¤: {output.flatten()}")
    else:
        print(f"  æœ€åˆã®5è¦ç´ : {output.flatten()[:5]}")
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:**

- **å‹•çš„å½¢çŠ¶ã®å‡¦ç†**: ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›å½¢çŠ¶ã«`None`ã‚„`-1`ãŒå«ã¾ã‚Œã‚‹å ´åˆã€å®Ÿéš›ã®å€¤ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- **ãƒ‡ãƒ¼ã‚¿å‹ã®ä¸€è‡´**: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‹ãŒãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹å‹ã¨ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚„å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æ³¨æ„ãŒå¿…è¦ã§ã™

#### è¤‡æ•°å…¥åŠ›ãƒ»è¤‡æ•°å‡ºåŠ›ã®å‡¦ç†

```python
import onnxruntime as ort
import numpy as np

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
session = ort.InferenceSession("multi_input_model.onnx")

# å…¥åŠ›æƒ…å ±ã®å–å¾—
input_names = [input.name for input in session.get_inputs()]
input_shapes = [input.shape for input in session.get_inputs()]

# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
inputs = {}
for i, (name, shape) in enumerate(zip(input_names, input_shapes)):
    inputs[name] = np.random.randn(*shape).astype(np.float32)

# æ¨è«–ã®å®Ÿè¡Œ
outputs = session.run(None, inputs)

# çµæœã®è¡¨ç¤º
for i, output in enumerate(outputs):
    print(f"å‡ºåŠ›{i+1}ã®å½¢çŠ¶: {output.shape}")
```

### 1.2.2 C++ã§ã®ONNX Runtimeä½¿ç”¨

#### åŸºæœ¬çš„ãªæ¨è«–ã®å®Ÿè¡Œ

```cpp
#include <onnxruntime_cxx_api.h>
#include <vector>
#include <iostream>

int main() {
    // ç’°å¢ƒã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "ONNXTutorial");
    Ort::SessionOptions session_options;
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
    const char* model_path = "model.onnx";
    Ort::Session session(env, model_path, session_options);
    
    // ãƒ¡ãƒ¢ãƒªæƒ…å ±ã®å–å¾—
    Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(
        OrtArenaAllocator, OrtMemTypeDefault);
    
    // å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    std::vector<float> input_data(1 * 3 * 224 * 224);  // ãƒãƒƒãƒã‚µã‚¤ã‚º1, ãƒãƒ£ãƒ³ãƒãƒ«3, 224x224
    std::fill(input_data.begin(), input_data.end(), 0.5f);
    
    std::vector<int64_t> input_shape{1, 3, 224, 224};
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, input_data.data(), input_data.size(),
        input_shape.data(), input_shape.size());
    
    // å…¥åŠ›ãƒ»å‡ºåŠ›åã®å–å¾—
    const char* input_names[] = {"input"};
    const char* output_names[] = {"output"};
    
    // æ¨è«–ã®å®Ÿè¡Œ
    auto outputs = session.Run(Ort::RunOptions{nullptr}, 
                              input_names, &input_tensor, 1,
                              output_names, 1);
    
    // çµæœã®å–å¾—
    float* output_data = outputs[0].GetTensorMutableData<float>();
    auto output_shape = outputs[0].GetTensorTypeAndShapeInfo().GetShape();
    
    std::cout << "æ¨è«–ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ" << std::endl;
    std::cout << "å‡ºåŠ›å½¢çŠ¶: ";
    for (auto dim : output_shape) {
        std::cout << dim << " ";
    }
    std::cout << std::endl;
    
    return 0;
}
```

## 1.3 ONNX Runtimeã®æ§‹ç¯‰

### 1.3.1 æ§‹ç¯‰ã®æ–¹æ³•

#### ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ“ãƒ«ãƒ‰

ç‰¹å®šã®ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã¦ONNX Runtimeã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹å ´åˆã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã§ãã¾ã™ã€‚

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone --recursive https://github.com/microsoft/onnxruntime.git
cd onnxruntime

# ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
./build.sh --config Release --build_shared_lib --parallel

# Pythonç”¨ãƒ›ã‚¤ãƒ¼ãƒ«ã®æ§‹ç¯‰
./build.sh --config Release --build_wheel --parallel
```

#### Dockerç’°å¢ƒã§ã®æ§‹ç¯‰

```dockerfile
FROM ubuntu:20.04

RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    python3 \
    python3-pip

WORKDIR /workspace
RUN git clone --recursive https://github.com/microsoft/onnxruntime.git
WORKDIR /workspace/onnxruntime

RUN ./build.sh --config Release --build_shared_lib --parallel
```

### 1.3.2 ONNX Runtime APIæ¦‚è¦

#### Python API ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

```python
import onnxruntime as ort

# 1. InferenceSession - ãƒ¡ã‚¤ãƒ³ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
session = ort.InferenceSession("model.onnx")

# 2. SessionOptions - ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
options = ort.SessionOptions()
options.intra_op_num_threads = 4
options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

# 3. RunOptions - å®Ÿè¡Œæ™‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³
run_options = ort.RunOptions()
run_options.log_severity_level = 2

# 4. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
providers = [
    'CUDAExecutionProvider',  # GPUä½¿ç”¨
    'CPUExecutionProvider'    # CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
]
session = ort.InferenceSession("model.onnx", providers=providers)
```

#### ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—

```python
# å…¥åŠ›æƒ…å ±ã®å–å¾—
for input in session.get_inputs():
    print(f"å…¥åŠ›å: {input.name}")
    print(f"å½¢çŠ¶: {input.shape}")
    print(f"ãƒ‡ãƒ¼ã‚¿å‹: {input.type}")

# å‡ºåŠ›æƒ…å ±ã®å–å¾—
for output in session.get_outputs():
    print(f"å‡ºåŠ›å: {output.name}")
    print(f"å½¢çŠ¶: {output.shape}")
    print(f"ãƒ‡ãƒ¼ã‚¿å‹: {output.type}")

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å–å¾—
prof = session.end_profiling()
print(f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {prof}")
```

### 1.3.3 APIè©³ç´°

#### ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è©³ç´°è¨­å®š

```python
import onnxruntime as ort

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä½œæˆ
options = ort.SessionOptions()

# ä¸¦åˆ—å‡¦ç†ã®è¨­å®š
options.intra_op_num_threads = 8  # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼å†…ä¸¦åˆ—æ•°
options.inter_op_num_threads = 4  # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼é–“ä¸¦åˆ—æ•°

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
options.enable_mem_pattern = True
options.enable_cpu_mem_arena = True

# ã‚°ãƒ©ãƒ•æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
options.log_severity_level = 2  # 0:è©³ç´°, 1:æƒ…å ±, 2:è­¦å‘Š, 3:ã‚¨ãƒ©ãƒ¼, 4:è‡´å‘½çš„

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®æœ‰åŠ¹åŒ–
options.enable_profiling = True

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ
session = ort.InferenceSession("model.onnx", options)
```

#### å®Ÿè¡Œæ™‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```python
# å®Ÿè¡Œæ™‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š
run_options = ort.RunOptions()
run_options.log_severity_level = 1
run_options.log_verbosity_level = 0

# ã‚¿ã‚°ã‚’è¨­å®šã—ã¦å®Ÿè¡Œã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
run_options.run_tag = "inference_batch_1"

# æ¨è«–ã®å®Ÿè¡Œ
outputs = session.run(None, inputs, run_options)
```

### 1.4 å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–¢é€£API

#### ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†

```python
import onnxruntime as ort

# åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèª
available_providers = ort.get_available_providers()
print(f"åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")

# ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®å–å¾—
if 'CUDAExecutionProvider' in available_providers:
    print("CUDAå¯¾å¿œãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    # GPUæƒ…å ±ã®å–å¾—ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    try:
        import torch
        if torch.cuda.is_available():
            print(f"GPUæ•°: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    except ImportError:
        print("PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è©³ç´°ãªGPUæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“")
```

#### ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†

```python
# ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
def print_model_info(session):
    """ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º"""
    print("=== ãƒ¢ãƒ‡ãƒ«æƒ…å ± ===")
    
    # åŸºæœ¬æƒ…å ±
    try:
        metadata = session.get_modelmeta()
        print(f"ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼: {metadata.producer_name}")
        print(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {metadata.version}")
        print(f"èª¬æ˜: {metadata.description}")
    except:
        print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # å…¥åŠ›æƒ…å ±
    print("\n--- å…¥åŠ›æƒ…å ± ---")
    for i, input_meta in enumerate(session.get_inputs()):
        print(f"å…¥åŠ› {i+1}:")
        print(f"  åå‰: {input_meta.name}")
        print(f"  å½¢çŠ¶: {input_meta.shape}")
        print(f"  å‹: {input_meta.type}")
    
    # å‡ºåŠ›æƒ…å ±
    print("\n--- å‡ºåŠ›æƒ…å ± ---")
    for i, output_meta in enumerate(session.get_outputs()):
        print(f"å‡ºåŠ› {i+1}:")
        print(f"  åå‰: {output_meta.name}")
        print(f"  å½¢çŠ¶: {output_meta.shape}")
        print(f"  å‹: {output_meta.type}")

# ä½¿ç”¨ä¾‹
session = ort.InferenceSession("model.onnx")
print_model_info(session)
```

## ã¾ã¨ã‚

ã“ã®ç« ã§ã¯ã€ONNX Runtimeã®åŸºç¤ã‹ã‚‰å®Ÿè·µçš„ãªä½¿ç”¨æ–¹æ³•ã¾ã§ã‚’æ¦‚è¦³ã—ã¾ã—ãŸã€‚

### ğŸ¯ å­¦ç¿’ã—ãŸå†…å®¹

1. **ONNX Runtimeã®æ¦‚è¦**
   - ç•°ãªã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§ã®ãƒ¢ãƒ‡ãƒ«å…±æœ‰ã‚’å®Ÿç¾ã™ã‚‹æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
   - ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã®é«˜æ€§èƒ½æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
   - CPUã€GPUã€å°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã§ã®æœ€é©åŒ–å®Ÿè¡Œ

2. **ç’°å¢ƒæ§‹ç¯‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   - Pythonï¼ˆpipï¼‰ã€C#ï¼ˆNuGetï¼‰ã€C++ï¼ˆã‚½ãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ï¼‰ã§ã®å°å…¥æ–¹æ³•
   - GPUå¯¾å¿œã®è¨­å®šã¨CUDAç’°å¢ƒã®è¦ä»¶
   - ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

3. **åŸºæœ¬çš„ãªæ¨è«–å®Ÿè¡Œ**
   - ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€æ¨è«–å®Ÿè¡Œã€çµæœå–å¾—ã¾ã§ã®æµã‚Œ
   - å‹•çš„å½¢çŠ¶ã‚„ãƒ‡ãƒ¼ã‚¿å‹ã®é©åˆ‡ãªå‡¦ç†
   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•

4. **é«˜åº¦ãªAPIæ´»ç”¨**
   - ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠã¨æœ€é©åŒ–
   - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¨æ€§èƒ½æ¸¬å®š

### ğŸ’¡ å®Ÿè·µã®ãƒã‚¤ãƒ³ãƒˆ

- **é–‹ç™ºæ®µéš**: Python APIã§è¿…é€Ÿãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
- **æœ¬ç•ªé‹ç”¨**: C++ã‚„C# APIã§é«˜æ€§èƒ½ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- **æœ€é©åŒ–**: é©åˆ‡ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
- **ãƒ‡ãƒãƒƒã‚°**: è©³ç´°ãªãƒ­ã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®æ´»ç”¨

### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æ¬¡ç« ã§ã¯ã€ã•ã‚‰ã«é«˜åº¦ãªONNX Runtimeã®æ©Ÿèƒ½ã‚’æ‰±ã„ã¾ã™ã€‚
- ã‚«ã‚¹ã‚¿ãƒ ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ä½œæˆ
- ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®åˆ†æ•£æ¨è«–
- ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã¨æœ€é©åŒ–
- å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºäº‹ä¾‹

ONNX Runtimeã¯ã€ç ”ç©¶ã‹ã‚‰æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã¾ã§å¹…åºƒãæ´»ç”¨ã§ãã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã“ã®åŸºç¤çŸ¥è­˜ã‚’åœŸå°ã¨ã—ã¦ã€æ¬¡ç« ã§ã‚ˆã‚Šå®Ÿè·µçš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºæŠ€è¡“ã‚’ç¿’å¾—ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

---

> **ğŸ“š è¿½åŠ å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹**
> - [ONNX Runtimeå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://onnxruntime.ai/docs/)
> - [ONNXãƒ¢ãƒ‡ãƒ«ã‚ºãƒ¼](https://github.com/onnx/models)
> - [ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒ³ãƒ—ãƒ«](https://github.com/microsoft/onnxruntime/tree/main/samples)
